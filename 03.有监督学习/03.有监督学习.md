## 03.有监督学习 ##

### 一、理论 ###

#### 1. 广义线性模型 ####

线性回归、岭回归、套索回归和对数几率回归见周志华老师西瓜书。

#### 2.K临近 ####

见周志华老师西瓜书。

#### 3.朴素贝叶斯 ####

见周志华老师西瓜书。

#### 4.决策树 ####

见周志华老师西瓜书。

#### 5.支持向量机 ####

见周志华老师西瓜书。

### 二、有监督学习对比 ###

接下来将解决一个回归问题和一个分类问题。为了避免过拟合问题，数据分为测试集和训练集。然而，也许还有必要使用第3个集合--验证集，优化超参数(hyperparameter，比如SVM的C和![](https://i.imgur.com/hZYyFG7.png)、岭回归的α)。原始数据也许太小，分为三部分不合适。此外、算法的结果也许也会收到训练集、验证集和测试集所选用得到特定数据点的影响。这两个问题常用的解决方法是使用交叉验证方法评估模型--将数据分为k个子集（叫作折），并按照如下方式训练模型：

- 用k - 1折数据作为训练数据训练模型。
- 用剩余数据测试得到的模型。
- 重复上面步骤k次（一开始确定的折数），每次用另外k - 1折数据训练（因此每次使用的测试集也就不同）。对于k次迭代得到的正确率取均值，得到最终的正确率。

#### 1.回归问题 ####

下面将结合波士顿郊区房数据集评估各种回归算法的优劣。房屋数据集有13个特征：

![](https://i.imgur.com/tMchOm9.png)

![](https://i.imgur.com/CwWATcJ.png)

为了评估模型的质量，这里计算模型的均方误差和判定系数R^2（coefficient of determination）。R^2的定义为：

![](https://i.imgur.com/XO8KKdZ.png)

其中，![](https://i.imgur.com/BQnQyPC.png)表示模型给出的预测标签。

数据部分如下图所示：

![](https://i.imgur.com/4Rt8p2v.png)

如果模型完美你和数据，R^2 = 1，模型的预测效果最佳。而R^2 = 0时，模型为一条恒等的直线（如果为负数，那就表明拟合程度很糟糕）。下面代码，训练线性回归、岭回归、索套回归和SVM回归模型，并计算各自的均方误差和判定系数，这里使用了sklearn库：

	import pandas as pd
	import numpy as np
	from sklearn import cross_validation
	from sklearn import svm
	from sklearn.tree import DecisionTreeRegressor
	from sklearn.ensemble import RandomForestRegressor
	from sklearn.linear_model import LinearRegression
	from sklearn.linear_model import Ridge
	from sklearn.linear_model import Lasso
	from sklearn.neighbors import KNeighborsRegressor
	from sklearn.metrics import mean_squared_error

	df = pd.read_csv('housing.csv',sep=',',header=None)
	#shuffle the data
	df = df.iloc[np.random.permutation(len(df))]
	X= df[df.columns[:-1]].values
	Y = df[df.columns[-1]].values
	
	cv = 10
	print 'linear regression'
	lin = LinearRegression()
	scores = cross_validation.cross_val_score(lin, X, Y, cv=cv)
	print("mean R2: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
	predicted = cross_validation.cross_val_predict(lin, X,Y, cv=cv)
	print 'MSE:',mean_squared_error(Y,predicted)
	
	print 'ridge regression'
	ridge = Ridge(alpha=1.0)
	scores = cross_validation.cross_val_score(ridge, X, Y, cv=cv)
	print("mean R2: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
	predicted = cross_validation.cross_val_predict(ridge, X,Y, cv=cv)
	print 'MSE:',mean_squared_error(Y,predicted)
	
	print 'lasso regression'
	lasso = Lasso(alpha=0.1)
	scores = cross_validation.cross_val_score(lasso, X, Y, cv=cv)
	print("mean R2: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
	predicted = cross_validation.cross_val_predict(lasso, X,Y, cv=cv)
	print 'MSE:',mean_squared_error(Y,predicted)
	
	print 'decision tree regression'
	tree = DecisionTreeRegressor(random_state=0)
	scores = cross_validation.cross_val_score(tree, X, Y, cv=cv)
	print("mean R2: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
	predicted = cross_validation.cross_val_predict(tree, X,Y, cv=cv)
	print 'MSE:',mean_squared_error(Y,predicted)
	
	print 'random forest regression'
	forest = RandomForestRegressor(n_estimators=50, max_depth=None,min_samples_split=1, 
	                               random_state=0)
	scores = cross_validation.cross_val_score(forest, X, Y, cv=cv)
	print("mean R2: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
	predicted = cross_validation.cross_val_predict(forest, X,Y, cv=cv)
	print 'MSE:',mean_squared_error(Y,predicted)
	
	#svm
	print 'linear support vector machine'
	svm_lin = svm.SVR(epsilon=0.2,kernel='linear',C=1)
	scores = cross_validation.cross_val_score(svm_lin, X, Y, cv=cv)
	print("mean R2: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
	predicted = cross_validation.cross_val_predict(svm_lin, X,Y, cv=cv)
	print 'MSE:',mean_squared_error(Y,predicted)
	
	print 'support vector machine rbf'
	clf = svm.SVR(epsilon=0.2,kernel='rbf',C=1.)
	scores = cross_validation.cross_val_score(clf, X, Y, cv=cv)
	print("mean R2: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
	predicted = cross_validation.cross_val_predict(clf, X,Y, cv=cv)
	print 'MSE:',mean_squared_error(Y,predicted)
	
	print 'knn'
	knn = KNeighborsRegressor()
	scores = cross_validation.cross_val_score(knn, X, Y, cv=cv)
	print("mean R2: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
	predicted = cross_validation.cross_val_predict(knn, X,Y, cv=cv)
	print 'MSE:',mean_squared_error(Y,predicted)

运行结果如下：

	linear regression
	mean R2: 0.71 (+/- 0.21)
	MSE: 24.0444410779
	ridge regression
	mean R2: 0.71 (+/- 0.21)
	MSE: 24.1692938533
	lasso regression
	mean R2: 0.70 (+/- 0.20)
	MSE: 25.0850940931
	decision tree regression
	mean R2: 0.76 (+/- 0.21)
	MSE: 19.0018181818
	random forest regression
	mean R2: 0.87 (+/- 0.11)
	MSE: 10.5741786008
	linear support vector machine
	mean R2: 0.69 (+/- 0.23)
	MSE: 26.2299464922
	support vector machine rbf
	mean R2: -0.01 (+/- 0.09)
	MSE: 83.9542473215
	knn
	mean R2: 0.53 (+/- 0.26)
	MSE: 38.4419003953

随机森林算法（50棵树）训练的模型对数据的拟合程度最高：它返回的判定系数均值为0.86，MSE=11.5。决策树回归符合预期，它的R^2比随机森林小，MSE则比随机森林高（这两个指标分别为0.67和25）.内核rbf（C=1,![](https://i.imgur.com/hZYyFG7.png)=0.2）的支持向量机是最差的模型，其MSE高达83.9，R^2则为0.0.二线性内核SVM是一个非常出色的模型（R^2和MSE分别为0.69和25.8）。套索回归和岭回归结果相差不大，R^2和MSE分别约为0.7和24.提升模型效果的一个重要方法是特征提取。因为在所有特征中，往往只有一部分特征适用于模型训练，而其他特征也对模型的R^2没有任何贡献。特征选取也许会改善R^2，因为将起误导作用的数据排除在外，同时训练时间也会相应缩短（考虑的特征会更少）。

为特定模型抽取最佳特征，方法有很多，这里将探索递归特征消除法(Recrusive Feature Elimination, RSE)，它只选取具有最大绝对权重的特征，直到特征的数量满足为止。SVM算法，权重就是w的值，而回归算法，权重为模型的参数![](https://i.imgur.com/Oe12gRl.png)。接下来利用sklearn的内置函数RFE，仅用四个最佳特征(best_feature)来观察模型的预测效果：

	from sklearn.feature_selection import RFE
	best_features=4
	print 'feature selection on linear regression'
	rfe_lin = RFE(lin,best_features).fit(X,Y)
	mask = np.array(rfe_lin.support_)
	scores = cross_validation.cross_val_score(lin, X[:,mask], Y, cv=cv)
	print("R2: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
	predicted = cross_validation.cross_val_predict(lin, X[:,mask],Y, cv=cv)
	print 'MSE:',mean_squared_error(Y,predicted)
	
	print 'feature selection ridge regression'
	rfe_ridge = RFE(ridge,best_features).fit(X,Y)
	mask = np.array(rfe_ridge.support_)
	scores = cross_validation.cross_val_score(ridge, X[:,mask], Y, cv=cv)
	print("R2: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
	predicted = cross_validation.cross_val_predict(ridge, X[:,mask],Y, cv=cv)
	print 'MSE:',mean_squared_error(Y,predicted)
	
	print 'feature selection on lasso regression'
	rfe_lasso = RFE(lasso,best_features).fit(X,Y)
	mask = np.array(rfe_lasso.support_)
	scores = cross_validation.cross_val_score(lasso, X[:,mask], Y, cv=cv)
	print("R2: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
	predicted = cross_validation.cross_val_predict(lasso, X[:,mask],Y, cv=cv)
	print 'MSE:',mean_squared_error(Y,predicted)
	
	print 'feature selection on decision tree'
	rfe_tree = RFE(tree,best_features).fit(X,Y)
	mask = np.array(rfe_tree.support_)
	scores = cross_validation.cross_val_score(tree, X[:,mask], Y, cv=cv)
	print("R2: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
	predicted = cross_validation.cross_val_predict(tree, X[:,mask],Y, cv=cv)
	print 'MSE:',mean_squared_error(Y,predicted)
	
	print 'feature selection on random forest'
	rfe_forest = RFE(forest,best_features).fit(X,Y)
	mask = np.array(rfe_forest.support_)
	scores = cross_validation.cross_val_score(forest, X[:,mask], Y, cv=cv)
	print("R2: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
	predicted = cross_validation.cross_val_predict(forest, X[:,mask],Y, cv=cv)
	print 'MSE:',mean_squared_error(Y,predicted)
	
	print 'feature selection on linear support vector machine'
	rfe_svm = RFE(svm_lin,best_features).fit(X,Y)
	mask = np.array(rfe_svm.support_)
	scores = cross_validation.cross_val_score(svm_lin, X[:,mask], Y, cv=cv)
	print("R2: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
	predicted = cross_validation.cross_val_predict(svm_lin, X,Y, cv=cv)
	print 'MSE:',mean_squared_error(Y,predicted)

结果如下：

	feature selection on linear regression
	R2: 0.61 (+/- 0.31)
	MSE: 33.182126206
	feature selection ridge regression
	R2: 0.61 (+/- 0.32)
	MSE: 33.2543979822
	feature selection on lasso regression
	R2: 0.68 (+/- 0.20)
	MSE: 27.4174043724
	feature selection on decision tree
	R2: 0.70 (+/- 0.35)
	MSE: 24.1185968379
	feature selection on random forest
	R2: 0.84 (+/- 0.14)
	MSE: 13.6755712332
	feature selection on linear support vector machine
	R2: 0.60 (+/- 0.33)
	MSE: 25.833801836

RFE函数返回布尔值列表（用RFE函数的support_属性得到该列表），选择了那些特征（值为true）和没有选择那些特征（值为false）。我们选择的特征评估前面几个模型。

即使仅仅使用4个特征，最佳模型仍然是由50棵树组成的随机森林算法，它的R^2只是稍微低于用于全部特征训练得到的模型（0.82和0.86的差距）。其余模型--套索回归、岭回归、决策树和线性SVM回归--它们的R^2与随机森林有较大的差距，但比起之前用全部特征训练得到的模型结果依然可观。需要注意的是，KNN算法不支持为特征加权，所以无法使用REF方法抽取特征。