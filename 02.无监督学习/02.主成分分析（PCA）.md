## 主成分分析（PCA）##

降维也称作特征抽取，是指将高维数据空间转换为低维的数据空间的操作。降维得到的子空间应该包含于原始数据最相关的的信息，降维技术分为线性和非线性两大类。降维操作采用多种技术，从大型数据集抽取最相关信息，降低复杂度，同时保留相关的信息。最著名的将为主成分分析（PCA）将原始数据以线性形式映射到维度互不相关的子空间。

### PCA理论 ###

主成分分析算法旨在识别数据集相关信息所在的子空间上。实际上，由于某些数据维度的数据点存在相关性，因此PCA寻找的是没有相关性、数据不同的维度，这样的维度很少。

例如，汽车行驶轨迹可以用一系列变量描述，比如km/h或m/s表示的速度、用经纬度表示的位置、用离开给定多少米或者英里表示的位置。显然，我们可以对这些维度进行降维操作，因为不同的速度变量或者位置变量给出的信息相同（相关变量），因此相关的子空间可以由两个不相关的维度组成（速度变量和位置变量）。PCA不仅寻找不相关变量，还寻找方差最大的维度。也就是说，对于km/h和m/s表示的速度，该算法选择方差较大的变量，这两种速度表示之间的关系为velocity[km/h]=3.6*velocity[m/s]，函数图像如下图所示（表示这种函数关系的直线更加靠近km/h轴，因为1km/h=3.6m/s）。速度在km/h轴上的投影(projection)比起在m/s轴上的更加分散：

![](https://i.imgur.com/JZWElI2.png)

上图为用m/s和km/h表示的速度两者之间为线性函数关系。数据点在km/h轴上的投影方差较大，而m/s轴上的投影方差较小。沿直线velocity[km/h]=3.6*velocity[m/s]分布的数据点的方差要大于两条轴上投影的方差。

寻找方差最大化的不相关维度等价于按以下步骤进行。童谣，我们有特征向量![](https://i.imgur.com/F7Lb1LV.png)：

![](https://i.imgur.com/NYtbhS1.png)

![](https://i.imgur.com/urqDeqh.png)

最终，特征的向量（主成分）![](https://i.imgur.com/ehlIgwj.png)位于子空间![](https://i.imgur.com/sYEy30B.png)，它仍然保留着原始向量的最大方差（和信息）。

处理高维数据集，比如脸部识别，PCA降维技术非常有用。对于脸部识别任务，需要比对输入的图像和数据库中的其他图像，以找到正确的人。用PCA实现的Eigenfaces（特征脸）应用，利用每张图像中大量像素是相关的这一特点实现人脸识别。例如，图像背景处的像素都是相关（相同）的，因此可以用降维方法，在维度更少的子空间比较图像，可以更快的给出准确结果。

### PCA案例 ###

下面这个例子讲解PCA和NumPy库的用法。任务是识别二维数据集的主成分。该数据集中的数据点沿着直线y=2x分布，其中包含一些随机（正态分布）添加的噪声。数据集及其图像可以用下面代码实现：

	import numpy as np
	from matplotlib import pyplot as plt
	%matplotlib inline 

	#line y = 2*x
	x = np.arange(1,101,1).astype(float)
	y = 2*np.arange(1,101,1).astype(float)
	#add noise
	noise = np.random.normal(0, 10, 100)
	y += noise
	
	fig = plt.figure(figsize=(10,10))
	#plot
	plt.plot(x,y,'ro')
	plt.axis([0,102, -20,220])
	plt.quiver(60, 100,10-0, 20-0, scale_units='xy', scale=1)
	plt.arrow(60, 100,10-0, 20-0,head_width=2.5, head_length=2.5, fc='k', ec='k')
	plt.text(70, 110, r'$v^1$', fontsize=20)
	
	#save
	ax = fig.add_subplot(111)
	ax.axis([0,102, -20,220])
	ax.set_xlabel('x',fontsize=40)
	ax.set_ylabel('y',fontsize=40)
	fig.suptitle('2 dimensional dataset',fontsize=40)
	fig.savefig('pca_data.png')

下图就是生成的数据集，显然，数据沿着一定方向分布，它对应的就是我们需要从数据中抽取的主成分V^1：

![](https://i.imgur.com/DsEz8Ij.png)

该算法计算二维数据集的均值和均值漂移后得到的数据集的均值，然后用相应的标准差调整数据的取值范围：

	#calc PCA
	mean_x = np.mean(x)
	mean_y = np.mean(y)
	mean_vector = np.array([[mean_x],[mean_y]])
	u_x = (x- mean_x)/np.std(x)
	u_y = (y-mean_y)/np.std(y)
	sigma = np.cov([u_x,u_y])
	print sigma

sigma结果如下：

	[[ 1.01010101  0.99414118]
	 [ 0.99414118  1.01010101]]

为了抽取主成分，需要计算特征值和特征向量，选择特征值最大的特征向量：

	eig_vals, eig_vecs = np.linalg.eig(sigma)
	
	eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i])
	             for i in range(len(eig_vals))]
	             
	eig_pairs.sort()
	eig_pairs.reverse()
	print eig_pairs

结果如下：

	[(2.0042421934092753, array([ 0.70710678,  0.70710678])), (0.015959826792744769, array([-0.70710678,  0.70710678]))]

为了确认主成分是否按照预期沿直线分布，我们需要将其坐标调整回去：

	v1 = eig_pairs[0][1]
	#leading eigenvector:
	x_v1 = v1[0]*np.std(x)+mean_x
	y_v1 = v1[1]*np.std(y)+mean_y
	print 'slope:',(y_v1)/(x_v1)

结果如下：

	slope: 2.00213432259

计算得到斜率约为2，和一开始选取的数值一致。

scikit-learn库提供了一种快捷的PCA算法实现，无须调整数据取值范围或者移动均值。使用sklearn模块之前，我们需要将调整后的数据转换为矩阵格式，矩阵的每一行为用x、y坐标表示的数据点：

	from sklearn.decomposition import PCA
	X = np.array([u_x,u_y])
	X = X.T
	print X.shape

结果如下：

	(100L, 2L)

现在可以运行PCA模块，指定目标主成分数量（这里设置为1）：

	pca = PCA(n_components=1)
	pca.fit(X)
	V = pca.components_
	print V,'-',V[0][1]/V[0][0]

结果如下：

	[[ 0.70710678  0.70710678]] - 1.0

这种做法得到的主成分与之前自己计算得到的结果完全相同，因此直线的斜率也相同。接下来利用这两种方法将数据集转换到新的一维空间中：

	#transform in reduced space
	X_red_sklearn = pca.fit_transform(X)
	print X_red_sklearn.shape
	
	W = np.array(v1.reshape(2,1))
	X_red = W.T.dot(X.T)
	#check the reduced matrices are equal
	assert X_red.T.all() == X_red_sklearn.all(), 'problem with the pca algorithm'
	print X_red.T[0],'-',X_red_sklearn[0]

结果如下：

	(100L, 1L)
	[-2.35110921] - [-2.35110921]

assert断言语句没有抛出异常，所以两种方法的结果完全一致。