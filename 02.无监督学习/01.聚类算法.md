## 聚类算法 ##

### 一、理论 ###

#### 1.最大期望算法 ####

最大似然估计见： https://blog.csdn.net/zengxiantao1994/article/details/72787849

最大期望算法见： https://blog.csdn.net/zhihua_oba/article/details/73776553

#### 2.高斯混合聚类算法 ####

见周志华老师西瓜书。

#### 3.K均值算法 ####

见周志华老师西瓜书。

#### 4.均值漂移 ####

见周志华老师西瓜书。

#### 5.层次聚类 ####

见周志华老师西瓜书。

### 二、聚类方法的训练和对比 ###

首先生成一个数据集，比较前面的几种聚类算法。从均值为![](https://i.imgur.com/UrZMqXi.png)和![](https://i.imgur.com/7Wl89zg.png)![](https://i.imgur.com/k7tEUJ4.png)，方差为![](https://i.imgur.com/Z1aljbz.png)的两个二维正态分布，随机选取数据点形成数据集。

这里使用NumPy库生成数据点，用matplotlib绘制图像。

首先生成需要的数据点：

	from matplotlib import pyplot as plt
	import numpy as np
	%matplotlib inline
	
	#Training and comparison of the clustering methods
	np.random.seed(4711)  # for repeatability 
	c1 = np.random.multivariate_normal([10, 0], [[3, 1], [1, 4]], size=[100,])
	l1 = np.zeros(100)
	l2 = np.ones(100)
	c2 = np.random.multivariate_normal([0, 10], [[3, 1], [1, 4]], size=[100,])
	#add noise:
	np.random.seed(1)  # for repeatability 
	noise1x = np.random.normal(0,2,100)
	noise1y = np.random.normal(0,8,100)
	noise2 = np.random.normal(0,8,100)
	c1[:,0] += noise1x
	c1[:,1] += noise1y
	c2[:,1] += noise2
	
	fig = plt.figure(figsize=(20,15))
	ax = fig.add_subplot(111)
	ax.set_xlabel('x',fontsize=30)
	ax.set_ylabel('y',fontsize=30)
	fig.suptitle('classes',fontsize=30)
	labels = np.concatenate((l1,l2),)
	X = np.concatenate((c1, c2),)
	pp1= ax.scatter(c1[:,0], c1[:,1],cmap='prism',s=50,color='r')
	pp2= ax.scatter(c2[:,0], c2[:,1],cmap='prism',s=50,color='g')
	ax.legend((pp1,pp2),('class 1', 'class2'),fontsize=35)
	fig.savefig('classes.png')

![](https://i.imgur.com/nlShyaU.png)

上面为两个类别分别添加了服从正态分布的噪声，这样使例子看起来更加真实。

下面将使用sklearn和scipy库实现几种聚类算法，绘图还是使用matplotlib库：

	#start figure
	fig.clf()#reset plt
	
	from sklearn import mixture
	from scipy.cluster.hierarchy import linkage
	from scipy.cluster.hierarchy import fcluster
	from sklearn.cluster import KMeans
	from sklearn.cluster import MeanShift
	
	fig, ((axis1, axis2), (axis3, axis4)) = plt.subplots(2, 2, sharex='col', sharey='row')
	
	#k-means
	kmeans = KMeans(n_clusters=2)
	kmeans.fit(X)
	pred_kmeans = kmeans.labels_
	plt.scatter(X[:,0], X[:,1], c=kmeans.labels_, cmap='prism')  # plot points with cluster dependent colors
	axis1.scatter(X[:,0], X[:,1], c=kmeans.labels_, cmap='prism')
	axis1.set_ylabel('y',fontsize=40)
	axis1.set_title('k-means',fontsize=20)
	
	#mean-shift
	ms = MeanShift(bandwidth=7)
	ms.fit(X)
	pred_ms = ms.labels_
	axis2.scatter(X[:,0], X[:,1], c=pred_ms, cmap='prism')
	axis2.set_title('mean-shift',fontsize=20)
	
	#gaussian mixture
	g = mixture.GMM(n_components=2)
	g.fit(X) 
	pred_gmm = g.predict(X)
	axis3.scatter(X[:,0], X[:,1], c=pred_gmm, cmap='prism')
	axis3.set_xlabel('x',fontsize=40)
	axis3.set_ylabel('y',fontsize=40)
	axis3.set_title('gaussian mixture',fontsize=20)
	
	#hierarchical
	# generate the linkage matrix
	Z = linkage(X, 'ward')
	max_d = 110
	pred_h = fcluster(Z, max_d, criterion='distance')
	axis4.scatter(X[:,0], X[:,1], c=pred_h, cmap='prism')
	axis4.set_xlabel('x',fontsize=40)
	axis4.set_title('hierarchical ward',fontsize=20)
	fig.set_size_inches(18.5,10.5)
	fig.savefig('comp_clustering.png', dpi=100)

上述代码中，我们需要为k均值函数和高斯混合模型指定簇的数量（n_clusters=2、n_components=2），将均值漂移算法的带宽设置为7（bandwidth=7）。层次聚类算法使用Ward连接来定义距离，终止层次算法的最大（Ward链接）距离max_d，我们设置为110.fcluster函数预测每个数据点属于哪个簇。k均值和均值漂移方法的聚类和高斯混合方法使用fit函数训练，而层次聚类算法使用linkage函数训练。上述代码输出如下如所示：

![](https://i.imgur.com/78EfvaM.png)

均值漂移和层次聚类方法将数据分为两个簇，因此我们选取的参数值（带宽和最大距离）合适。需要注意的是，对于层次聚类算法，可以更具下面代码生成的树状图开选择足底啊距离参数的取值：

	fig.clf()#reset plt
	
	from scipy.cluster.hierarchy import dendrogram
	fig = plt.figure(figsize=(20,15))
	plt.title('Hierarchical Clustering Dendrogram',fontsize=30)
	plt.xlabel('data point index (or cluster index)',fontsize=30)
	plt.ylabel('distance (ward)',fontsize=30)
	dendrogram(
	    Z,
	    truncate_mode='lastp',  # show only the last p merged clusters
	    p=12,
	    leaf_rotation=90.,
	    leaf_font_size=12.,
	    show_contracted=True,
	)
	fig.savefig('dendrogram.png')

![](https://i.imgur.com/pMdcier.png)

通过truncate_mode='lastp'标记可指定将最后多少次合并绘制成图（该例中P2）.上图表明，当距离在100到135之间是，只剩下两个簇。上图横坐标标签（小括号中的数字）表示最后12次合并之前每个簇所包含的数据点的数量。

